# Cynaptics Induction Task

With the rapid advancements in Artificial Intelligence, models are becoming increasingly powerful, and their outputs are becoming harder to distinguish from real-world data. This is particularly evident in the field of image generation. Cutting-edge models like **Flux Dev** and **SD3.5** produce such realistic images that differentiating them from genuine ones has become a significant challenge.

## Task Objective

The goal of this competition is to develop a **machine learning algorithm** capable of distinguishing between real and AI-generated images.

## Key Details

- **Freedom of Approach**: Participants are free to initiate the task using any algorithm of their choice.
- **Bonus Challenge**: Implement a **GAN (Generative Adversarial Network)** and use it for classifying images as either real or AI-generated.

## Provided Resources

- **Training Data**: Training data has been provided to support the development of models.
- **Baseline Scripts**:
  - A baseline script for the classification task.
  - A baseline script for GAN implementation.
  - Participants are encouraged to explore, modify, and experiment with these scripts to achieve better results.
- **References**:
  - https://www.geeksforgeeks.org/feature-extraction-in-image-processing-techniques-and-applications/
  - https://blog.roboflow.com/what-is-resnet-50/
  - https://www.youtube.com/watch?v=TpMIssRdhco&pp=ygUTVW5kZXJzdGFuZGluZyBhIEdBTg%3D%3D
  - https://aws.amazon.com/what-is/gan/

- Submit a **CSV file** containing:
  - The **ID** of each test image.
  - The **predicted label** generated by your algorithm.
  - The **github repo** containing your code.


# AI vs Real

The baseline script for a basic AI vs Real Classification is as follow:

```python
# -*- coding: utf-8 -*-
"""aivsreal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wi6sfmWO7Kl68-Q94OXwq6EoIRz854Rc
"""

from keras.utils import to_categorical
from keras_preprocessing.image import load_img
from keras.models import Sequential
from keras.applications import MobileNetV2, ResNet152, VGG16, EfficientNetB0, InceptionV3
from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D
import os
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm

def createdataframe(dir):
    image_paths = []
    labels = []
    for label in os.listdir(dir):
        for imagename in os.listdir(os.path.join(dir, label)):
            image_paths.append(os.path.join(dir, label, imagename))
            labels.append(label)
        print(label, "completed")
    return image_paths, labels

def extract_features(images):
    features = []
    for image in tqdm(images):
        img = load_img(image, target_size=(224, 224))
        img = np.array(img)
        features.append(img)
    features = np.array(features)
    features = features.reshape(features.shape[0], 224, 224, 3)  # Reshape all images in one go
    return features

TRAIN_DIR = "/kaggle/input/dataset/Train"

train = pd.DataFrame()
train['image'], train['label'] = createdataframe(TRAIN_DIR)

train_features = extract_features(train['image'])

x_train = train_features / 255.0

le = LabelEncoder()
le.fit(train['label'])
y_train = le.transform(train['label'])
y_train = to_categorical(y_train, num_classes=2)

model = Sequential()
# Convolutional layers
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(236, 236, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(1024, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(2048, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(x=x_train, y=y_train, batch_size=25, epochs=20)
```

# GAN Implementation

The baseline script for training your GAN is as follow:

```python
# -*- coding: utf-8 -*-
"""Cynaptics Induction Task GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aw2ClOjrtQORLguCyLa0bqBP5vXq-TVo
"""

import pdb
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision import datasets, transforms
import torchvision.datasets as datasets
from torchvision.utils import make_grid

transform = transforms.Compose([transforms.Resize(255),
                                 transforms.CenterCrop(224),
                                 transforms.ToTensor()])

dataset = datasets.ImageFolder('path/to/data', transform=transform)

def show(tensor, ch=1, size=(512, 512), num=25):
    data = tensor.detach().cpu().view(-1, ch, *size)
    grid = make_grid(data[:num], nrow=5).permute(1, 2, 0)
    plt.imshow(grid)
    plt.show()

epoch = 1000
cur_iter = 0
info_iter = 300
mean_gen_loss = 0
mean_disc_loss = 0

z_dim = 64
lr = 0.0001
loss = nn.BCEWithLogitsLoss()
batch_size = 1024
device = "cuda"

dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)

def genBlock(inp_nodes, out_nodes):
    return nn.Sequential(
        nn.Linear(inp_nodes, out_nodes),
        nn.BatchNorm1d(out_nodes),
        nn.ReLU()
    )

def gen_noise(batch_size, z_dim):
    return torch.randn(batch_size, z_dim).to(device)

class Generator(nn.Module):
    def __init__(self, z_dim=64, o_dim=262144, h_dim=120):
        super().__init__()
        self.z_dim = z_dim
        self.o_dim = o_dim
        self.h_dim = h_dim
        self.gen = nn.Sequential(
            genBlock(z_dim, h_dim),
            genBlock(h_dim, h_dim * 2),
            genBlock(h_dim * 2, h_dim * 4),
            genBlock(h_dim * 4, h_dim * 8),
            genBlock(h_dim * 8, o_dim),
            nn.Sigmoid(),
        )

    def forward(self, noise):
        return self.gen(noise)

def discBlock(inp_nodes, out_nodes):
    return nn.Sequential(
        nn.Linear(inp_nodes, out_nodes),
        nn.LeakyReLU(0.2)
    )

class Discriminator(nn.Module):
    def __init__(self, inp_dim=262144, hidden_dim=256):
        super().__init__()
        self.inp_dim = inp_dim
        self.hidden_dim = hidden_dim
        self.disc = nn.Sequential(
            discBlock(inp_dim, hidden_dim * 4),
            discBlock(hidden_dim * 4, hidden_dim * 2),
            discBlock(hidden_dim * 2, hidden_dim),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, image):
        return self.disc(image)

# Optimizers
gen = Generator(z_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)

disc = Discriminator().to(device)
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)

def gen_loss(loss_func, gen, disc, batch_size, z_dim):
    noise = gen_noise(batch_size, z_dim)
    fake = gen(noise)
    pred = disc(fake)
    target = torch.ones_like(pred)
    return loss_func(pred, target)

def disc_loss(loss_func, gen, disc, batch_size, z_dim, real):
    noise = gen_noise(batch_size, z_dim)
    fake = gen(noise)
    disc_fake = disc(fake.detach())
    disc_fake_target = torch.zeros_like(disc_fake)
    disc_fake_loss = loss_func(disc_fake, disc_fake_target)

    disc_real = disc(real)
    disc_real_target = torch.ones_like(disc_real)
    disc_real_loss = loss_func(disc_real, disc_real_target)

    return (disc_fake_loss + disc_real_loss) / 2

for epoch in range(epoch):
    mean_disc_loss_list = []
    mean_gen_loss_list = []
    iters_list = []

    for real_image, _ in tqdm(dataloader):
        disc_opt.zero_grad()
        cur_batch_size = len(real_image)
        real_image = real_image.view(cur_batch_size, -1).to(device)
        disc_losses = disc_loss(loss, gen, disc, cur_batch_size, z_dim, real_image)
        disc_losses.backward()
        disc_opt.step()

        gen_opt.zero_grad()
        gen_losses = gen_loss(loss, gen, disc, cur_batch_size, z_dim)
        gen_losses.backward()
        gen_opt.step()

        mean_disc_loss += disc_losses.item() / info_iter
        mean_gen_loss += gen_losses.item() / info_iter
        mean_disc_loss_list.append(mean_disc_loss)
        mean_gen_loss_list.append(mean_gen_loss)

        if cur_iter % info_iter == 0 and cur_iter > 0:
            fake_noise = gen_noise(cur_batch_size, z_dim)
            fake = gen(fake_noise)
            show(real_image)
            show(fake)
            print(f"{epoch} : step {cur_iter}, Generator loss : {mean_gen_loss}, Discriminator Loss : {mean_disc_loss}")
            mean_gen_loss, mean_disc_loss = 0, 0

        iters_list.append(cur_iter)
        cur_iter += 1



---

